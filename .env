# 앱 기본 설정
APP_NAME=Local LLM API
API_VERSION=v1
DEBUG=False

# 서버 설정
HOST=0.0.0.0
PORT=8000
WORKERS=1

# CORS 설정
CORS_ORIGINS=["*"]

# 모델 백엔드 설정
# "llama-cpp", "transformers", "auto" 중 선택
MODEL_BACKEND=auto

# GGUF 모델 설정
MODEL_DIR=./models
DEFAULT_MODEL=llama3-8b-instruct.Q4_K_M.gguf
PRELOAD_MODEL=True

# Transformers 모델 설정
TRANSFORMERS_DEFAULT_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0

# LLM 설정
CONTEXT_SIZE=4096
GPU_LAYERS=-1
MAX_TOKENS=2048
TEMPERATURE=0.7
TOP_P=0.95
TOP_K=40

# 로깅 설정
LOG_LEVEL=INFO
LOG_FILE=./logs/api.log

# 성능 설정
REQUEST_TIMEOUT=300
MAX_CONCURRENT_REQUESTS=5

# 캐싱 설정
ENABLE_CACHE=True
CACHE_TTL=3600

# 보안 설정
API_KEY_REQUIRED=False
# API_KEY=your_secret_api_key_here